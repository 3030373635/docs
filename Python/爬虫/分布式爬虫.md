---
title: 分布式爬虫
---


https://www.cnblogs.com/linhaifeng/articles/8359774.html

https://www.cnblogs.com/Mint-diary/p/9728435.html

## 1.1 介绍

```
所谓分布式：使用多台机器搭建一个分布式集群，运行同一组程序，对目标站点资源联合爬取

原生scrapy无法完成分布式？
	- 调度器无法被分布式集群共享
	- 管道无法被共享

如何实现？
	- 将调度器共享
	- 将管道共享(其实只要一旦弄了集群，每个节点爬取的数据都不一样，也都可以存在本地，最后汇总，这个管道可选可不选)


原来scrapy的Scheduler（调度器）维护的是本机的任务队列（存放Request对象及其回调函数等信息）+本机的去重队列（存放访问过的url地址）

现在使用scrapy-redis后，这两个队列在redis中了，也就是基于内存的了。配置一下redfis，其他节点就能访问了，这就共享了


```

## 1.1 快速上手使用

```python
# 1 pip3 install scrapy-redis
# 2 原来继承Spider，现在继承RedisSpider(from scrapy_redis.spiders import RedisSpider)
# 3 不在需要allowed_domains，start_urls
# 4 需要写redis_key = 'myspider:start_urls'
# 5 setting中配置：

  # redis的连接
  REDIS_HOST = 'localhost'                            # 主机名
  REDIS_PORT = 6380                                   # 端口
  REDIS_PARAMS  = {'password':密码}
    # 使用scrapy-redis的去重
  DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
  # 使用scrapy-redis的Scheduler
  SCHEDULER = "scrapy_redis.scheduler.Scheduler"
  # 持久化的可以配置，也可以不配置
  ITEM_PIPELINES = {
     'scrapy_redis.pipelines.RedisPipeline': 299
  }


#现在要让爬虫运行起来，需要去redis中以myspider:start_urls为key，插入一个起始地址
127.0.0.1:6380> lpush myspider:start_urls https://www.cnblogs.com/
```

## 1.2 Duplication Filter

> Scrapy中用集合实现这个request去重功能，Scrapy中把已经发送的request指纹放入到一个集合中，把下一个request的指纹拿到集合中比对，如果该指纹存在于集合中，说明这个request发送过了，如果没有则继续操作。这个核心的判重功能是这样实现的：

```python
from scrapy.dupefilters import  RFPDupeFilter
class RFPDupeFilter:
  def request_seen(self, request):
        fp = self.request_fingerprint(request)
      	# 判断是否在集合中
        if fp in self.fingerprints:
            return True
        self.fingerprints.add(fp)
        if self.file:
            self.file.write(fp + '\n')

  def request_fingerprint(self, request):
    return request_fingerprint(request) # from scrapy.utils.request import request_fingerprint

  
from scrapy.utils.request import request_fingerprint
def request_fingerprint(request):
  return ... # 哈希过后的一串字符串
```

**test.py**

```python
from scrapy.utils.request import request_fingerprint
from scrapy import Request


r1 = Request("http://www.baidu.com/?name=meng&age=12")
r2 = Request("http://www.baidu.com/?age=12&name=meng")
a = request_fingerprint(r1)
b = request_fingerprint(r2)
print(a) # ff5cf7295f94c97df6d24b0ea46ff2c1875b5c1d
print(b) # ff5cf7295f94c97df6d24b0ea46ff2c1875b5c1d

# 我们可以看出scrapy是将请求转为了一串字符串，并且参数位置不同，字符串结果还是一样的，scrapy-redis
```

```
在scrapy-redis中去重是由Duplication Filter组件来实现的，它通过redis的set 不重复的特性，巧妙的实现了Duplication Filter去重。scrapy-redis调度器从引擎接受request，将request的指纹存⼊redis的set检查是否重复，并将不重复的request push写⼊redis的 request queue。

引擎请求request(Spider发出的）时，调度器从redis的request queue队列⾥里根据优先级pop 出⼀个request 返回给引擎，引擎将此request发给spider处理。
```

## 1.3 增量爬虫

```
什么是增量爬虫？
	当爬取结束后，如果目标站点更新资源了，我们还需要对更新的部分再做爬取
	如果在启动一次爬虫程序，那么又会重新在完整的爬取一次，这样就很鸡肋了，那么如何实现，只爬取更新部分

如何实现？
	对爬取的url进行检测，使用一张记录表存储爬取过的url，每次爬取前在表中寻找一下，如果有就代表爬过，没有就可以爬了